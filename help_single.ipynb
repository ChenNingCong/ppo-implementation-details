{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "0 18.0 21.0\n",
      "10 30.0 89.0\n",
      "20 44.523809523809526 76.66666666666667\n",
      "30 57.04 115.0\n",
      "40 108.16 153.2\n",
      "50 160.44 171.16666666666666\n",
      "60 260.76 170.71428571428572\n",
      "70 337.24 211.875\n",
      "80 369.84 243.88888888888889\n",
      "90 366.16 269.5\n",
      "100 399.12 285.54545454545456\n",
      "110 454.32 303.4166666666667\n",
      "120 451.24 318.53846153846155\n",
      "130 478.2 331.5\n",
      "140 500.0 342.73333333333335\n",
      "150 500.0 352.5625\n",
      "160 499.2 353.70588235294116\n",
      "170 480.04 361.8333333333333\n",
      "180 452.08 360.2105263157895\n",
      "190 453.0 367.2\n",
      "200 462.84 373.5238095238095\n",
      "210 500.0 379.27272727272725\n",
      "220 496.16 384.5217391304348\n",
      "230 496.16 389.3333333333333\n",
      "240 500.0 393.76\n",
      "250 500.0 412.92\n",
      "260 500.0 426.64\n",
      "270 494.64 444.56\n",
      "280 494.64 455.36\n",
      "290 493.88 463.12\n",
      "300 482.76 472.68\n",
      "310 464.84 485.96\n",
      "320 467.12 485.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     trainer\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m wrapper()\n",
      "\u001b[1;32m/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/chenningcong/Desktop/ppo-implementation-details/help_single.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Desktop/ppo-implementation-details/ppo_single_env.py:149\u001b[0m, in \u001b[0;36mPPOTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m obs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(obs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    148\u001b[0m rollout_obs\u001b[39m.\u001b[39mappend(obs)\n\u001b[0;32m--> 149\u001b[0m action, action_log_prob, entropy, state_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mget_action_and_value(obs\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), action\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    150\u001b[0m actions\u001b[39m.\u001b[39mappend(action\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    151\u001b[0m action_log_probs\u001b[39m.\u001b[39mappend(action_log_prob\u001b[39m.\u001b[39mitem())          \n",
      "File \u001b[0;32m~/Desktop/ppo-implementation-details/ppo_single_env.py:52\u001b[0m, in \u001b[0;36mAgent.get_action_and_value\u001b[0;34m(self, x, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_action_and_value\u001b[39m(\u001b[39mself\u001b[39m, x, action\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor(x)\n\u001b[0;32m---> 52\u001b[0m     probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mCategorical(logits\u001b[39m=\u001b[39;49mlogits)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m         action \u001b[39m=\u001b[39m probs\u001b[39m.\u001b[39msample()\n",
      "File \u001b[0;32m~/Desktop/DeepLearning/deep/lib/python3.10/site-packages/torch/distributions/categorical.py:64\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`logits` parameter must be at least one-dimensional.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[39m# Normalize\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits \u001b[39m=\u001b[39m logits \u001b[39m-\u001b[39m logits\u001b[39m.\u001b[39;49mlogsumexp(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobs \u001b[39mif\u001b[39;00m probs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from ppo_single_env import *\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = Agent(env=env)\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "trainer = PPOTrainer(\n",
    "    env = env,\n",
    "    agent = agent,\n",
    "    rollout_num = 100000,\n",
    "    rollout_length = None,\n",
    "    epsilon = 0.2,\n",
    "    value_loss_weight = 0.2,\n",
    "    clip_norm = 0.5,\n",
    "    norm_adv = True,\n",
    "    norm_return = True,\n",
    "    ppo_epoch = 5,\n",
    "    ppo_num_minibatch = None,\n",
    "    gamma = 0.99,\n",
    "    use_gae = False,\n",
    "    lam = 0.95\n",
    ")\n",
    "def wrapper():\n",
    "    trainer.train()\n",
    "wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenningcong/Desktop/DeepLearning/deep/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/chenningcong/Desktop/ppo-implementation-details/video/1.mp4 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/chenningcong/Desktop/ppo-implementation-details/video/1.mp4/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/chenningcong/Desktop/ppo-implementation-details/video/1.mp4/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/chenningcong/Desktop/ppo-implementation-details/video/1.mp4/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-360.5485551234823"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env =gym.wrappers.RecordVideo(gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\"), f\"video/1.mp4\")\n",
    "@torch.no_grad\n",
    "def validate(self, new_env):\n",
    "    # this returns undiscounted reward\n",
    "    old_env = self.env\n",
    "    try:\n",
    "        self.env = new_env  # set new environment\n",
    "        obs, _ = self.env.reset()\n",
    "        # add a batch dim\n",
    "        obs = torch.tensor(obs, device=self.device).unsqueeze(0)\n",
    "        rewards = []\n",
    "        with torch.no_grad():\n",
    "            self.agent.eval()\n",
    "            total_reward = 0\n",
    "            terminated = truncated = False \n",
    "            while not (terminated or truncated):\n",
    "                action, action_log_prob, entropy, state_value = self.agent.get_action_and_value(obs, action=None)\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                # # we disable truncation...\n",
    "                # assert not np.any(truncated)\n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "                obs = torch.tensor(obs, device=self.device).unsqueeze(0) \n",
    "    finally:\n",
    "        self.env.close()\n",
    "        self.env = old_env\n",
    "    return total_reward\n",
    "validate(trainer, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RecordVideo<TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLander-v2>>>>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
